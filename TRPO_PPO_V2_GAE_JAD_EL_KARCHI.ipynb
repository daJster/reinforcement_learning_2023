{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46aaa60b-2e51-4ccc-963d-7693e0184485",
   "metadata": {},
   "source": [
    "# Reinforcement Learning 2023 : TRPO + PPO v2 + GAE\n",
    "## Author : Jad El Karchi\n",
    "This is an implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1edd982-bd85-4622-8496-98dd1ceb3cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Iteration 100, Total Reward: -0.500572919845581\n",
      "Iteration 200, Total Reward: 1.3723833411931992\n",
      "Iteration 300, Total Reward: 0.3609631210565567\n",
      "Iteration 400, Total Reward: -0.06682701408863068\n",
      "Iteration 500, Total Reward: -0.339872345328331\n",
      "Iteration 600, Total Reward: -0.7321433275938034\n",
      "Iteration 700, Total Reward: 0.9249025136232376\n",
      "Iteration 800, Total Reward: -0.19070617854595184\n",
      "Iteration 900, Total Reward: -0.2717653959989548\n",
      "Iteration 1000, Total Reward: -0.0396607369184494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
    "# Function to compute the Hessian-vector product using Conjugate Gradient\n",
    "def conjugate_gradient(Ax, b, max_iter=10, tol=1e-10):\n",
    "\n",
    "    x = np.zeros_like(b)\n",
    "    r = tf.identity(b)\n",
    "    p = tf.identity(r)\n",
    "    rho = np.dot(r, r)\n",
    "\n",
    "    # to understand !\n",
    "    for _ in range(max_iter):\n",
    "        z = Ax(p)\n",
    "        alpha = rho / (np.dot(p, z) + 1e-8)\n",
    "        x += alpha * p\n",
    "        r -= alpha * z\n",
    "        rho_new = np.dot(r, r)\n",
    "        beta = rho_new / (rho + 1e-8)\n",
    "        p = r + beta * p\n",
    "        rho = rho_new\n",
    "\n",
    "        if rho < tol:\n",
    "            break\n",
    "\n",
    "    return x\n",
    "\n",
    "# Function to run policy and collect trajectories\n",
    "def run_policy(env, policy, num_trajectories):\n",
    "    states, actions, rewards, log_probs = [], [], [], []\n",
    "\n",
    "    for _ in range(num_trajectories):\n",
    "        state = env.reset()[0]\n",
    "\n",
    "        while True:\n",
    "            # Sample action from the policy\n",
    "            action_prob = policy(tf.expand_dims(state, axis=0))[0]\n",
    "            action = np.random.choice(len(action_prob), p=action_prob.numpy())\n",
    "\n",
    "            log_prob = tf.math.log(action_prob[action])\n",
    "            next_state, reward, done, _ = env.step(action)[0]\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(log_probs)\n",
    "\n",
    "\n",
    "# Function to compute advantage estimates\n",
    "def calculate_advantages(rewards, gamma):\n",
    "    advantages = np.zeros_like(rewards, dtype=np.float32)\n",
    "    running_add = 0\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        running_add += rewards[t]*gamma\n",
    "        advantages[t] = running_add - np.mean(rewards)\n",
    "\n",
    "    return advantages\n",
    "\n",
    "def calculate_gae_advantages(rewards, values, gamma=0.99, lam=0.95):\n",
    "    deltas = rewards[:-1] + gamma * values[1:] - values[:-1]\n",
    "    advantages = np.zeros_like(rewards, dtype=np.float32)\n",
    "    advantage = 0.0\n",
    "\n",
    "    for t in reversed(range(len(deltas))):\n",
    "        advantage = deltas[t] + gamma * lam * advantage\n",
    "        advantages[t] = advantage\n",
    "\n",
    "    return advantages\n",
    "\n",
    "# Function to compute policy gradient\n",
    "def compute_policy_gradient(policy, states, actions, advantages):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = policy(states)\n",
    "        action_masks = tf.one_hot(actions, depth=logits.shape[1])\n",
    "        selected_logits = tf.reduce_sum(action_masks * logits, axis=1)\n",
    "        loss = -tf.reduce_sum(selected_logits * advantages)\n",
    "        \n",
    "    return tape.gradient(loss, policy.trainable_variables)\n",
    "\n",
    "# Main TRPO training loop\n",
    "def trpo_training(env, policy, num_iterations=100, num_trajectories=10, max_timesteps=1000, gamma=0.99):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    for iteration in range(1, num_iterations + 1):\n",
    "        # Run policy and collect trajectories\n",
    "        states, actions, rewards, log_probs = run_policy(env, policy, num_trajectories)\n",
    "\n",
    "        # Estimate advantage function\n",
    "        advantages = calculate_advantages(rewards, gamma)\n",
    "\n",
    "        # Compute policy gradient\n",
    "        policy_gradient = compute_policy_gradient(policy, states, actions, advantages)\n",
    "\n",
    "        # Use CG to compute F^-1 g\n",
    "        def hessian_vector_product(vector):\n",
    "            with tf.GradientTape() as tape2:\n",
    "                with tf.GradientTape() as tape1:\n",
    "                    logits = policy(states)\n",
    "                    action_masks = tf.one_hot(actions, depth=logits.shape[1])\n",
    "                    selected_logits = tf.reduce_sum(action_masks * logits, axis=1)\n",
    "                    kl_divergence = tf.reduce_sum(tf.stop_gradient(tf.expand_dims(log_probs, axis=-1)) * (tf.expand_dims(log_probs, axis=-1) - tf.math.log(logits)), axis=1)\n",
    "\n",
    "                gradient = tape1.gradient(selected_logits, policy.trainable_variables)\n",
    "                gradient_vector_product = sum(tf.reduce_sum(g * v) for g, v in zip(gradient, vector))\n",
    "\n",
    "                surrogate_loss = tf.reduce_mean(-selected_logits * advantages)\n",
    "                kl_penalty = tf.reduce_mean(kl_divergence)\n",
    "\n",
    "            hessian_vector_product_grad = tape2.gradient(gradient_vector_product, policy.trainable_variables)\n",
    "\n",
    "            return hessian_vector_product_grad[3]\n",
    "\n",
    "        hvp = conjugate_gradient(hessian_vector_product, policy_gradient[3])\n",
    "\n",
    "        # Do line search on surrogate loss and KL constraint\n",
    "        # (Not implemented here; typically a backtracking line search is used)\n",
    "\n",
    "        # Update policy parameters\n",
    "        optimizer.apply_gradients(zip(hvp, policy.trainable_variables))\n",
    "\n",
    "        # Print or log some information\n",
    "        if iteration % 100 == 0:\n",
    "            total_reward = sum(rewards)\n",
    "            print(f\"Iteration {iteration}, Total Reward: {total_reward}\")\n",
    "\n",
    "# Define a simple policy network\n",
    "class Policy(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(Policy, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Create CartPole environment\n",
    "swimmer_env = gym.make('CartPole-v1')\n",
    "\n",
    "# Create policy network and optimizer\n",
    "policy_model = Policy(num_actions=swimmer_env.action_space.n)\n",
    "\n",
    "# Run TRPO training\n",
    "trpo_training(swimmer_env, policy_model, num_iterations=10**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0190c2e-96df-4544-a022-690e7965b6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_v1_training(env, policy, value_network, num_iterations=100, num_trajectories=10, max_timesteps=1000, epochs=10, clip_ratio=0.2, beta=0.01, gamma=0.99):\n",
    "    optimizer_policy = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    optimizer_value = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    for iteration in range(1, num_iterations + 1):\n",
    "        # Run policy and collect trajectories\n",
    "        states, actions, rewards, log_probs = run_policy(env, policy, num_trajectories)\n",
    "\n",
    "        # Train value network\n",
    "        for epoch in range(epochs):\n",
    "            values = value_network(states)\n",
    "            advantages = calculate_advantages(rewards, gamma) #values)\n",
    "            with tf.GradientTape() as tape_value:\n",
    "                value_loss = tf.reduce_mean(tf.square(values - (rewards + advantages)))\n",
    "            gradients_value = tape_value.gradient(value_loss, value_network.trainable_variables)\n",
    "            optimizer_value.apply_gradients(zip(gradients_value, value_network.trainable_variables))\n",
    "\n",
    "        # Train policy network using PPO objective\n",
    "        for epoch in range(epochs):\n",
    "            with tf.GradientTape() as tape_policy:\n",
    "                new_log_probs = policy(states)\n",
    "                ratio = tf.exp(new_log_probs - log_probs)\n",
    "                clipped_ratio = tf.clip_by_value(ratio, 1 - clip_ratio, 1 + clip_ratio)\n",
    "                surrogate = tf.minimum(ratio * advantages, clipped_ratio * advantages)\n",
    "                policy_loss = -tf.reduce_mean(surrogate)\n",
    "            gradients_policy = tape_policy.gradient(policy_loss, policy.trainable_variables)\n",
    "            optimizer_policy.apply_gradients(zip(gradients_policy, policy.trainable_variables))\n",
    "\n",
    "        # Dual descent update for beta\n",
    "        entropy = -tf.reduce_sum(policy(states) * tf.math.log(policy(states)))\n",
    "        with tf.GradientTape() as tape_beta:\n",
    "            beta_loss = -beta * tf.reduce_mean(entropy)\n",
    "        gradients_beta = tape_beta.gradient(beta_loss, policy.trainable_variables)\n",
    "        optimizer_policy.apply_gradients(zip(gradients_beta, policy.trainable_variables))\n",
    "\n",
    "        # Print or log some information\n",
    "        if iteration % 10 == 0:\n",
    "            total_reward = sum(rewards)\n",
    "            print(f\"Iteration {iteration}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6caf35a-491f-4869-bae1-80239b97ac8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
